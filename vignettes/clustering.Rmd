---
title: "Clustering"
output: 
   html_document:
    df_print: paged
---

```{r folding, include = F}

hooks = knitr::knit_hooks$get()

hook_foldable = function(type) {
  force(type)
  function(x, options) {
    res = hooks[[type]](x, options)
    
    if (isFALSE(options[[paste0("fold.", type)]])) return(res)
    
    paste0(
      "<details><summary>", type, "</summary>\n\n",
      res,
      "\n\n</details>"
    )
  }
}

knitr::knit_hooks$set(
  output = hook_foldable("output")
)

```

```{r setup, include = F}

knitr::opts_chunk$set(echo = T, fold.output = F, message = F, fig.show = "hold", out.width = "100%")

devtools::load_all()

# load and adjust object
object <- readRDS(file = "data/bids-week4.RDS")
object@cdata$cluster <- dplyr::select(object@cdata$cluster, cell_id)

# needed objects
kmeans_methods <- c("Hartigan-Wong", "Lloyd")

```

## Prerequisites

Make sure to be familiar with the following tutorials before proceeding: 

- [Variable Sets](variable-sets.html)

## 1. Introduction

There are a variety of clustering algorithms. Each of these algorithms comes with its own specific methods and options and metrics such as different distance measures (*euclidean*, *manhattan*) or agglomeration methods (*complete*, *ward.D*). As clustering is exploratory approach more than one way of clustering cells might be *correct* or at least provide a satisfying model. `cypro` currently implements *hierarchical*, *kmeans* and *partitioning around medoids (PAM)*. It does that in a convenient fashion that allows you to iterate over a variety of clustering options for a variety of variable sets and to store all results in the *cypro* object - ready to be plotted or extracted when needed. This tutorial introduces you to the workflow of all three clustering methods. 

```{r setup-vis, eval = F, echo = T}

# load packages
library(cypro)

# load object from broad institute compound profiling experiment week 4
# contains one time imaging data
object <- readRDS(file = "data/bids-week4.RDS")

saveCyproObject(object)

```

## 2. Hierarchical clustering 

Hierarchical clustering is one of the most common algorithms used in exploratory data analysis. It usually contains four steps: 

- 1. Decide which data variables you want to base the clustering on. 
- 2. Compute a distance matrix based on the data variables (several distance metrics exist). 
- 3. Agglomerate the observations to a hierarchical tree based on the distance matrix (several agglomeration metrics exist). 
- 4. Cut the hierarchical tree at a certain height or with a certain predefined value for k. 

`Cypro` implements these four steps with four functions. 

### 2.1 Initiation

To initiate hierarchical clustering means to set up the respective S4 object in your *cypro* object by determining the variables on which you want to base the clustering. This is done with 'cypro's variable set functionality. 

```{r example-vset, fold.output = T}

# show variables of a previously defined variable set
getVariableSet(object, variable_set = "intensity")

```

The function `initiateHierarchicalClustering()` sets up the necessary object within the *cypro* object. 

```{r initiate-hclust, eval = F}

object <- initiateHierarchicalClustering(object, variable_set = "intensity")

```

### 2.2 Compute distance matrices

To obtain all valid methods with which distance matrices can be computed use `validDistanceMethods()`. 

```{r valid-distance-methods}

validDistanceMethods()

```

While you can specify the distance methods one by one, a more convenient approach would be to tell `cypro` all distance metrics of interest and then let it do the computation for each one respectively in one function call. Luckily, this is exactly what makes `cypro` so convenient. 

```{r, eval = F}

distance_methods <- c("euclidean", "manhattan")

object <- computeDistanceMatrices(object, variable_set = "intensity", method_dist = distance_methods)

```


## 2.3 Agglomerate hierarchical cluster

There are a variety of methods to agglomerate distance measures to hierarchical tree. Again you can specify all approaches you would like to explore in one simple function call. Use `validAgglomerationMethods()` to obtain valid input options for argument `method_aggl`.

```{r valid-aggl-methods}

validAgglomerationMethods()

```

Specifying two different input options makes `agglomerateHierarchicalCluster()` use every unique combination automatically. 

```{r agglomerate-hclust, eval = F}

aggl_methods <- c("complete", "ward.D")

object <- agglomerateHierarchicalCluster(object,
                                         variable_set = "intensity",
                                         method_dist = distance_methods, 
                                         method_aggl = aggl_methods)

```

The function `plotDendrogram()` allows to visualize cluster agglomeration with a variety of options. Check out its documentation with `?plotDendrogram`.

```{r plot dendrogram, fig.show = "hold", out.width = "100%", cache = T, fig.cap = "Figure 1.1 Dendrogram visualizing one of the distance-agglomeration options"}

plotDendrogram(object, variable_set = "intensity", method_dist = "euclidean", method_aggl = "ward.D", k = 4)

```

### 2.4 Adding hierarchical cluster variables

To make interesting clustering results available for subsequent analysis steps they must be added in form of grouping variables to the [cluster data](extract_data.html#Cluster data.frame). If no variables have been added the data.frame is empty. 

```{r get cluster vars, message = T, fold.output = F}

# no clustering variables have been added yet
getClusterVariableNames(object)

```

Adding clustering variables can be done conveniently via `addHierarchicalClusterVariables()`. Specify distance methods, agglomeration methods and all ways according to which you want to cut the trees. All valid combinations are added to the 
cell cluster data and are made available for arguments like `across` or `grouping_variable`. 

```{r add-hclust-vars, eval = T}

object <- addHierarchicalClusterVariables(object, variable_set = "intensity", 
                                          method_dist = "euclidean", 
                                          method_aggl = c("complete", "ward.D"), 
                                          k = c(3,4,5)
                                          )

getClusterVariableNames(object)

```

### 2.5 A note on handling distance matrices

Distance matrices can become very big in size. The convenient iteration that `cypro` allows comes with a drawback: The *cypro* object grows in size pretty quick, too. 

```{r dist-size}

pryr::object_size(object)

hclust_conv <- getHclustConv(object, variable_set = "intensity")

euclidean_dist_mtr <- hclust_conv@dist_matrices$euclidean

pryr::object_size(euclidean_dist_mtr)

```

Fortunately, the computation of distance matrices and the agglomeration of hierarchical cluster are separated processes. Once cluster have been aggregated based on a distance matrix, the latter is not needed any longer. You can therefore discard them, even before adding the cluster variables to the cell data. 

```{r dist-size-discarded}

object <- discardDistanceMatrix(object, variable_set = "intensity", method_dist = "euclidean")

hclust_conv <- getHclustConv(object, variable_set = "intensity")

# does not exist any longer ( == NULL)
hclust_conv@dist_matrices$euclidean

# cypro object is way smaller 
pryr::object_size(object)

```
We recommend to regularly check the size of the *cypro* object with `pryr::object_size()` and to discard unneeded distance matrices. In case you realize after discarding that you need it again you can always compute it again via `computeDistanceMatrices()` without initiating a new hierarchical clustering for the variable set of interest!


## 3. Kmeans clustering 

Kmeans clustering can be conducted in a similar fashion. 

### 3.1 Initiatiation 

To initiate kmeans clustering based on a new variable set use `initiateKmeansClustering()`.

```{r initiate-kmeans, eval = F}

object <- initiateKmeansClustering(object, variable_set = "intensity")

```

### 3.2 Cluster computation

`validKmeansMethods()` provides all valid kmeans methods.
```{r valid-kmeans-methods}

validKmeansMethods()

```
The function `computeKmeansCluster()` iterates over all valid input combinations for arguments `k` and `method_kmeans`. 

```{r compute-kmeans-cluster, eval = F}

kmeans_methods <- c("Hartigan-Wong", "Lloyd")

object <- computeKmeansCluster(object,
                               variable_set = "intensity",
                               k = 2:10, # compute clustering for values k = 2, k = 3, ... k = 10
                               method_kmeans = kmeans_methods)

```

To assess clustering quality and optimal inputs for `k` use `plotScreeplot()`. Based on it's output you can decide with which clustering results you want to proceed.
```{r kmeans-screeplot, fig.show = "hold", out.width = "100%", fig.cap = "Figure 3.1 Screeplots can be used to determine optimal value for k"}

plotScreeplot(object,
              variable_set = "intensity", 
              k = 2:10, 
              method_kmeans = kmeans_methods)


```

### 3.4 Adding kmeans cluster variables

To make clustering results available for subsequent functions use `addKmeansClusteringVariables()`. 

```{r add-kmeans-vars}

object <- addKmeansClusterVariables(object, variable_set = "intensity", k = 3:5, method_kmeans = "Lloyd")

getClusterVariableNames(object, starts_with("kmeans"))

```


## 4. Partitioning around medoids (PAM)

The PAM algorigthm works very similar to kmeans although it is a bit more robust and comes with an additional metric, *silhouette width*, to assess cluster quality. A major drawback of this algorithm is, that it is way slower than kmeans. A recommended approach is to conduct kmeans clustering first to narrow down the values for argument `k` that come into question. Apart from that the PAM clustering workflow is equal to the one of kmeans. 

### 4.1 Initiate PAM clustering 

Use the function `initiatePamClustering()` to set up the analysis pipeline. 

```{r initiate-pam, eval = F}

object <- initiatePamClustering(object, variable_set = "intensity")

```

### 4.2 Run the algorithm

... with function `computePamCluster()`. 

```{r compute-pam-cluster, eval = F}

object <- computePamCluster(object, variable_set = "intensity", k = 2:10)

```

Partitioning around medoids comes with some plotting options to assess cluster quality such as the average silhouette width...

```{r plot-avg-sil-width, fig.cap = "Figure 4.1 Average silhouette width plot."}

plotAvgSilhouetteWidths(object, variable_set = "intensity", k = 2:10)

```

... and the silhouette widths for every observation.

```{r plot-sil-width, cache = T, fig.cap = "Figure 4.2 Silhouette width plot."}

plotSilhouetteWidths(object, variable_set = "intensity", k = 2:10, ncol = 3)

```

### 4.3 Add PAM cluster variables 

Add the cluster  variables of interest via `addPamClusterVariables()`. 

```{r add-pam-vars}

object <- addPamClusterVariables(object, variable_set = "intensity", k = c(4,5))

getClusterVariableNames(object, contains("pam"))

```

## Renaming cluster variables and cluster groups

To ensure unambiguous naming with respect to algorithm, variable set, methods etc. the initial clustering names are somewhat bulky. In addition to that are cluster named by number which is barely informative. In case you want to rename cluster variables or single cluster groups of a variable have a look at the tutorial about how to [rename content](rename-content.html) in `cypro`.
